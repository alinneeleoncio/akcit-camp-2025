# ============================================================
# TechAdvisor â€“ Seu primeiro agente com LangChain + LangGraph
# ============================================================
# Objetivo didÃ¡tico:
# - Mostrar como conectar um LLM (OpenAI) usando a integraÃ§Ã£o moderna `langchain-openai`.
# - Ensinar a criar um `PromptTemplate` e compor uma pipeline com LCEL: `prompt | llm | parser`.
# - Demonstrar a orquestraÃ§Ã£o de um fluxo simples com LangGraph (`StateGraph`).
# - Rodar de forma interativa no terminal, recebendo um interesse e retornando uma recomendaÃ§Ã£o.

import os
from dotenv import load_dotenv
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END

# ============================================================
# 1. Carregar variÃ¡veis de ambiente
# ============================================================
# Busca um arquivo `.env` na raiz do projeto e carrega as variÃ¡veis
# (por exemplo, OPENAI_API_KEY). Assim, nÃ£o precisamos exportar
# manualmente no terminal a cada execuÃ§Ã£o.
load_dotenv()

# ============================================================
# 2. Definir o LLM (OpenAI via langchain-openai)
# ============================================================
# `ChatOpenAI` Ã© o wrapper do LangChain para modelos de chat da OpenAI.
# ParÃ¢metros principais:
# - model: nome do modelo (ajuste para o que sua conta permite).
# - temperature: controla a criatividade (0 = mais determinÃ­stico; 1 = mais criativo).
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7
)

# ============================================================
# 3. Definir o PromptTemplate
# ============================================================
# O PromptTemplate permite parametrizar trechos do prompt, como `{interesse}`.
# No momento de executar, substituiremos esse placeholder pelo valor fornecido
# pelo usuÃ¡rio no estado do LangGraph.
template_text = (
    "VocÃª Ã© um assistente que recomenda tecnologias de programaÃ§Ã£o "
    "com base no interesse do usuÃ¡rio.\n"
    "UsuÃ¡rio: {interesse}\n"
    "Responda em portuguÃªs e recomende uma tecnologia apropriada "
    "para o usuÃ¡rio aprender em seguida, explicando brevemente o porquÃª."
)
prompt = PromptTemplate(
    input_variables=["interesse"],
    template=template_text
)

# ============================================================
# 4. Criar uma pipeline LCEL (Prompt -> Modelo -> Parser)
# ============================================================
# LCEL (LangChain Expression Language) permite compor etapas como um pipeline.
# Aqui encadeamos:
#   1) prompt: recebe `{interesse}` e gera a string final de instruÃ§Ã£o
#   2) llm: chama o modelo de chat da OpenAI com esse prompt
#   3) StrOutputParser(): converte a resposta para uma string simples
chain = prompt | llm | StrOutputParser()

# ============================================================
# 5. Integrar a pipeline dentro de um fluxo com LangGraph
# ============================================================
# Usamos um grafo de estado (StateGraph) onde cada nÃ³ recebe e devolve
# um dicionÃ¡rio. Assim fica fÃ¡cil adicionar mÃºltiplos passos no futuro.

# DefiniÃ§Ã£o do estado (cada nÃ³ lÃª e atualiza esse dicionÃ¡rio)
def techadvisor_node(state: dict):
    # ExtraÃ­mos o interesse do usuÃ¡rio que foi inserido no estado
    interesse = state["interesse"]

    # Executamos a pipeline LCEL passando a variÃ¡vel do prompt
    resposta = chain.invoke({"interesse": interesse})

    # Gravamos a saÃ­da no estado para outros nÃ³s (ou a etapa final) consumirem
    state["resposta"] = resposta
    return state

# Criar o grafo do agente
graph = StateGraph(dict)

# Adiciona um nÃ³ "recomendador" que utiliza a pipeline definida acima
graph.add_node("recomendador", techadvisor_node)

# Define o ponto inicial e o final do fluxo
# Entrada -> recomendador -> END
graph.set_entry_point("recomendador")
graph.add_edge("recomendador", END)

# Compila o grafo em um executor (cria um app pronto para .invoke)
app = graph.compile()

# ============================================================
# 6. ExecuÃ§Ã£o interativa (simulaÃ§Ã£o de uso)
# ============================================================
# Loop de CLI simples para interagir com o agente.
# A cada entrada do usuÃ¡rio, invocamos o grafo passando um estado inicial
# com a chave "interesse" e depois exibimos a chave "resposta".
if __name__ == "__main__":
    print("ğŸ¤– TechAdvisor - Recomenda tecnologias com base em seus interesses!\n")
    while True:
        interesse = input("O que vocÃª quer aprender ou melhorar? (ou 'sair'): ")
        if interesse.lower() in ["sair", "exit", "quit"]:
            print("Encerrando o agente. AtÃ© logo!")
            break
        # Executa o fluxo LangGraph com o estado inicial contendo o interesse
        result = app.invoke({"interesse": interesse})
        print("\nğŸ” Resposta do agente:")
        print(result["resposta"])
        print("-" * 60)
